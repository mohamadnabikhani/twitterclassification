{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149048\n",
      "107230\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from os import path\n",
    "# d = path.dirname(__file__)\n",
    "d = \"/home/mohammad/nlpproject/tweet\"\n",
    "STOPWORD = set([i for i in open((path.join(d, 'stopword.txt'))).read().split('\\n')])\n",
    "\n",
    "principlist_text = open('renew_principlist.txt', 'r').read().decode('utf-8')\n",
    "reformist_text = open('renew_reformist.txt', 'r').read().decode('utf-8')\n",
    "\n",
    "principlist_words = principlist_text.split()\n",
    "new_principlist_words = []\n",
    "reformist_words = reformist_text.split()\n",
    "new_reformist_words = []\n",
    "\n",
    "for i in principlist_words:\n",
    "    if i.endswith('ها'.decode('utf-8')):\n",
    "        new_principlist_words.append(i.rsplit('ها'.decode('utf-8'),1)[0])\n",
    "        new_principlist_words.append('ها'.decode('utf-8'))\n",
    "    elif i.endswith('های'.decode('utf-8')):\n",
    "        new_principlist_words.append(i.rsplit('های'.decode('utf-8'),1)[0])\n",
    "        new_principlist_words.append('های'.decode('utf-8'))\n",
    "#     principlist_words.remove(principlist_words[])\n",
    "    else:\n",
    "        new_principlist_words.append(i)\n",
    "#----------------------------------------\n",
    "for i in reformist_words:\n",
    "    if i.endswith('ها'.decode('utf-8')):\n",
    "        new_reformist_words.append(i.rsplit('ها'.decode('utf-8'),1)[0])\n",
    "        new_reformist_words.append('ها'.decode('utf-8'))\n",
    "    elif i.endswith('های'.decode('utf-8')):\n",
    "        new_reformist_words.append(i.rsplit('های'.decode('utf-8'),1)[0])\n",
    "        new_reformist_words.append('های'.decode('utf-8'))\n",
    "#     principlist_words.remove(principlist_words[])\n",
    "    else:\n",
    "        new_reformist_words.append(i)\n",
    "        \n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "# print(isEnglish(u'mechanism'))\n",
    "\n",
    "for i in new_principlist_words:\n",
    "    if i in STOPWORD:\n",
    "        new_principlist_words.remove(i)\n",
    "    elif isEnglish(i):\n",
    "        new_principlist_words.remove(i)\n",
    "#-----------------------------------------\n",
    "for i in new_reformist_words:\n",
    "    if i in STOPWORD:\n",
    "        new_reformist_words.remove(i)\n",
    "    elif isEnglish(i):\n",
    "        new_reformist_words.remove(i)\n",
    "        \n",
    "print len(new_reformist_words)\n",
    "print len(new_principlist_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149048\n",
      "107230\n",
      "1787 2484\n",
      "صهیونست ها از اول با آدمکشى سرزمین فلسطینی ها را غصب کردند و با تشدید نفى بلد و نسل کشى تلاش کردند استقرار بناحق خود را تثبیت کنند آن ها به اشتباه گمان مى کنند با ادامه آدمکشى و ارعاب و ظلم مى توانند خود را بر منطقه تحمیل و ماندگار کنند مسئولان قواى سه گانه کشور باید بکوشند با\n",
      "-------------------\n",
      "امروز بعدازظهر وزیر اطلاعات برای پاسخگویی به دو سؤال نمایندگان در کمیسیون امنیت و سیاست خارجی حاضر خواهد شد ۱ علت برخورد با خانم فروهر و تعلل در تعقیب و مجازات قاتلان والدین او ۲ ضوابط تهیه ، انتشار ، توزیع و مراقبت از بولتن‌ ها محرمانه به نظر من تحمل مجازات فرار و سرزنش افکار عمومی بسیار سخت‌تر از\n"
     ]
    }
   ],
   "source": [
    "reformist_tweets = []\n",
    "print len(new_reformist_words)\n",
    "for i in range(len(new_reformist_words)/60):\n",
    "    reformist_tweets.append(' '.join(new_reformist_words[60 * i:60* i + 60]))\n",
    "    \n",
    "principlist_tweets = []\n",
    "print len(new_principlist_words)\n",
    "for i in range(len(new_principlist_words)/60):\n",
    "    principlist_tweets.append(' '.join(new_principlist_words[60 * i:60* i + 60]))\n",
    "    \n",
    "print len(principlist_tweets), len(reformist_tweets)\n",
    "print principlist_tweets[0]\n",
    "print \"-------------------\"\n",
    "print reformist_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features : two word,  three word,  four word \n",
    "economic = ['دلار'.decode('utf-8'), 'ریال'.decode('utf-8'), 'ارز'.decode('utf-8'), 'گرانی'.decode('utf-8')]\n",
    "government = ['دولت'.decode('utf-8'), 'روحانی'.decode('utf-8'), 'وزیر'.decode('utf-8'), 'رییس'.decode('utf-8')\n",
    "             , 'جمهور'.decode('utf-8')]\n",
    "leadership = ['رهبر'.decode('utf-8'), 'رهبری'.decode('utf-8'), 'ولایت'.decode('utf-8')]\n",
    "foregin_policy = ['اسراییل'.decode('utf-8'), 'اسرائیل'.decode('utf-8'), 'فلسطین'.decode('utf-8')\n",
    "                 , 'سوریه'.decode('utf-8'),  'لبنان'.decode('utf-8'), 'آمریکا'.decode('utf-8'), 'ترامپ'.decode('utf-8')]\n",
    "\n",
    "country = ['کشور'.decode('utf-8'),'ایران'.decode('utf-8')]\n",
    "people = ['ملت'.decode('utf-8'),'جامعه'.decode('utf-8'),'مردم'.decode('utf-8')]\n",
    "female = ['زن'.decode('utf-8'),'زنان'.decode('utf-8'),'خانوم'.decode('utf-8'), 'دختر'.decode('utf-8'),\n",
    "         'دختران'.decode('utf-8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348\n"
     ]
    }
   ],
   "source": [
    "g = 0\n",
    "for i in new_principlist_words:\n",
    "    if i in foregin_policy:\n",
    "        g+=1\n",
    "print g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(type_feature ,arr_type, sentence_splited):\n",
    "    desired_feature = 0\n",
    "    for word in arr_type:\n",
    "        if word in sentence_splited:\n",
    "            index = sentence_splited.index(word)\n",
    "            if index >2 and index < len(sentence_splited) - 3:\n",
    "                desired_feature = \"{0}={1}\".format(type_feature, ' '.join(sentence_splited[index-2:index+2]).encode('utf-8'))\n",
    "            elif index < 2 :\n",
    "                desired_feature = \"{0}={1}\".format(type_feature, ' '.join(sentence_splited[index:index+4]).encode('utf-8'))\n",
    "            else:\n",
    "                desired_feature = \"{0}={1}\".format(type_feature, ' '.join(sentence_splited[index-4:index]).encode('utf-8'))\n",
    "#             reformist_economic = \"economic={0}\".format('')\n",
    "            break\n",
    "    if not desired_feature:\n",
    "        desired_feature = \"{0}={1}\".format(type_feature, \"NUL\")\n",
    "    return desired_feature.decode('utf-8')\n",
    "    \n",
    "final_data = []\n",
    "reformist_counter = 0\n",
    "for sentence in reformist_tweets:\n",
    "    temp_arr = []\n",
    "    reformist_id = \"reformist{0}\".format(reformist_counter)\n",
    "    reformist_economic,reformist_government,reformist_leadership,reformist_foregin_policy = 0,0,0,0\n",
    "    temp_arr.append(reformist_id)\n",
    "    temp_arr.append('reformist')\n",
    "    \n",
    "    sentence_splited = sentence.split()\n",
    "    \n",
    "    reformist_economic = get_feature(\"economic\", economic, sentence_splited)\n",
    "    reformist_government = get_feature(\"government\", government, sentence_splited)\n",
    "    reformist_leadership = get_feature(\"leadership\", leadership, sentence_splited)\n",
    "    reformist_foregin_policy = get_feature(\"foregin_policy\", foregin_policy, sentence_splited)\n",
    "    reformist_country = get_feature(\"country\", country, sentence_splited)\n",
    "    reformist_people = get_feature(\"people\", people, sentence_splited)\n",
    "    reformist_female = get_feature(\"female\", female, sentence_splited)\n",
    "    \n",
    "    temp_arr.append(reformist_economic)\n",
    "    temp_arr.append(reformist_government)\n",
    "    temp_arr.append(reformist_leadership)\n",
    "    temp_arr.append(reformist_foregin_policy)\n",
    "    temp_arr.append(reformist_country)\n",
    "    temp_arr.append(reformist_people)\n",
    "    temp_arr.append(reformist_female)\n",
    "    reformist_counter += 1\n",
    "    \n",
    "    temp_str = ' '.join(temp_arr) + '\\n'\n",
    "    final_data.append(temp_str)\n",
    "\n",
    "principlist_counter = 0\n",
    "for sentence in principlist_tweets:\n",
    "    temp_arr = []\n",
    "    principlist_id = \"principlist{0}\".format(principlist_counter)\n",
    "    principlist_economic,principlist_government,principlist_leadership, principlist_foregin_policy = 0,0,0,0\n",
    "    temp_arr.append(principlist_id)\n",
    "    temp_arr.append('principlist')\n",
    "    \n",
    "    sentence_splited = sentence.split()\n",
    "    \n",
    "    principlist_economic = get_feature(\"economic\", economic, sentence_splited)\n",
    "    principlist_government = get_feature(\"government\", government, sentence_splited)\n",
    "    principlist_leadership = get_feature(\"leadership\", leadership, sentence_splited)\n",
    "    principlist_foregin_policy = get_feature(\"foregin_policy\", foregin_policy, sentence_splited)\n",
    "    principlist_country = get_feature(\"country\", country, sentence_splited)\n",
    "    principlist_people = get_feature(\"people\", people, sentence_splited)\n",
    "    principlist_female = get_feature(\"female\", female, sentence_splited)\n",
    "    \n",
    "    temp_arr.append(principlist_economic)\n",
    "    temp_arr.append(principlist_government)\n",
    "    temp_arr.append(principlist_leadership)\n",
    "    temp_arr.append(principlist_foregin_policy)\n",
    "    temp_arr.append(reformist_country)\n",
    "    temp_arr.append(reformist_people)\n",
    "    temp_arr.append(reformist_female)\n",
    "    \n",
    "    principlist_counter += 1\n",
    "    \n",
    "    temp_str = ' '.join(temp_arr) + '\\n'\n",
    "    final_data.append(temp_str)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_file = open('mallet_data.txt', 'w')\n",
    "for i in final_data:\n",
    "    final_file.write(i.encode('utf-8'))\n",
    "final_file.close()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mohammad/mallet-2.0.8\n",
      "/home/mohammad/mallet-2.0.8\n",
      "Buildfile: /home/mohammad/mallet-2.0.8/build.xml\n",
      "\n",
      "init:\n",
      "     [copy] Copying 1 file to /home/mohammad/mallet-2.0.8/class\n",
      "\n",
      "compile:\n",
      "    [javac] /home/mohammad/mallet-2.0.8/build.xml:62: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds\n",
      "\n",
      "BUILD SUCCESSFUL\n",
      "Total time: 0 seconds\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "import os\n",
    "os.chdir('/home/mohammad/mallet-2.0.8')\n",
    "# os.chdir('/home/mohammad/nlpproject/tweet')\n",
    "!pwd\n",
    "!ant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buildfile: build.xml does not exist!\r\n",
      "Build failed\r\n"
     ]
    }
   ],
   "source": [
    "!cd /home/mohammad/mallet-2.0.8 | ant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unrecognized command: import_file\r\n",
      "Mallet 2.0 commands: \r\n",
      "\r\n",
      "  import-dir         load the contents of a directory into mallet instances (one per file)\r\n",
      "  import-file        load a single file into mallet instances (one per line)\r\n",
      "  import-svmlight    load SVMLight format data files into Mallet instances\r\n",
      "  info               get information about Mallet instances\r\n",
      "  train-classifier   train a classifier from Mallet data files\r\n",
      "  classify-dir       classify data from a single file with a saved classifier\r\n",
      "  classify-file      classify the contents of a directory with a saved classifier\r\n",
      "  classify-svmlight  classify data from a single file in SVMLight format\r\n",
      "  train-topics       train a topic model from Mallet data files\r\n",
      "  infer-topics       use a trained topic model to infer topics for new documents\r\n",
      "  evaluate-topics    estimate the probability of new documents under a trained model\r\n",
      "  prune              remove features based on frequency or information gain\r\n",
      "  split              divide data into testing, training, and validation portions\r\n",
      "  bulk-load          for big input files, efficiently prune vocabulary and import docs\r\n",
      "\r\n",
      "Include --help with any option for more information\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!bin/mallet import_file --input /home/mohammad/nlpproject/tweet/mallet_data.txt --output labeled_mallet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweet",
   "language": "python",
   "name": "tweet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
