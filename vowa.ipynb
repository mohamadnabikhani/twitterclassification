{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بعدازظهر\n"
     ]
    }
   ],
   "source": [
    "reformist_text = open('renew_reformist.txt', 'r').read().decode('utf-8')\n",
    "principlist_text = open('renew_principlist.txt', 'r').read().decode('utf-8')\n",
    "reformist_text_sentence = reformist_text.split()\n",
    "principlist_text_sentence = principlist_text.split()\n",
    "\n",
    "print reformist_text_sentence[1]\n",
    "reformist_text_array = []\n",
    "principlist_text_array = []\n",
    "\n",
    "\n",
    "for i in range(len(reformist_text_sentence)/25):\n",
    "    reformist_text_array.append(\" \".join(reformist_text_sentence[25*i:25*i+25]))\n",
    "    \n",
    "for i in range(len(principlist_text_sentence)/25):\n",
    "    principlist_text_array.append(\" \".join(principlist_text_sentence[25*i:25*i+25]))\n",
    "    \n",
    "def textToVW(lines):\n",
    "    b = []\n",
    "    for i in lines:\n",
    "        b.append(i.replace(':','COLON').replace('|','PIPE'))\n",
    "    return b\n",
    "reformist_text_array = textToVW(reformist_text_array)\n",
    "principlist_text_array = textToVW(principlist_text_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = ['+1 | text ' + s for s in reformist_text_array] + \\\n",
    "           ['-1 | text ' + s for s in principlist_text_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-++++-++-++++----+--++--++-+----+++-+-+++++-+++-++\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(1234)\n",
    "random.shuffle(examples)   # this does in-place shuffling\n",
    "# print out the labels of the first 50 examples to be sure they're sane:\n",
    "print ''.join([s[0] for s in examples[:50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8000 data/sentiment.tr\r\n",
      "   2016 data/sentiment.te\r\n",
      "  10016 total\r\n"
     ]
    }
   ],
   "source": [
    "def writeToVWFile(filename, examples):\n",
    "    with open(filename, 'w') as h:\n",
    "        for ex in examples:\n",
    "            print >>h, ex.encode('utf-8')\n",
    "            \n",
    "writeToVWFile('data/sentiment.tr', examples[:8000])\n",
    "writeToVWFile('data/sentiment.te', examples[8000:])\n",
    "valid_labels = [int(i[0:2]) for i in examples[8000:]]\n",
    "\n",
    "!wc -l data/sentiment.tr data/sentiment.te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !vw --binary data/sentiment.tr\n",
    "# !vw -d data/sentiment.tr --loss_function logistic -f data/sentiment.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !vw --binary data/sentiment.tr --passes 5 -c -k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = data/sentiment.model\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = data/sentiment.tr.cache\n",
      "Reading datafile = data/sentiment.tr\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0  -1.0000  -1.0000       27\n",
      "0.500000 1.000000            2            2.0   1.0000  -1.0000       27\n",
      "0.250000 0.000000            4            4.0   1.0000   1.0000       27\n",
      "0.250000 0.250000            8            8.0   1.0000   1.0000       27\n",
      "0.312500 0.375000           16           16.0  -1.0000  -1.0000       27\n",
      "0.343750 0.375000           32           32.0   1.0000   1.0000       27\n",
      "0.343750 0.343750           64           64.0  -1.0000   1.0000       27\n",
      "0.390625 0.437500          128          128.0   1.0000   1.0000       27\n",
      "0.335938 0.281250          256          256.0   1.0000   1.0000       27\n",
      "0.361328 0.386719          512          512.0   1.0000  -1.0000       27\n",
      "0.302734 0.244141         1024         1024.0  -1.0000  -1.0000       27\n",
      "0.272949 0.243164         2048         2048.0   1.0000   1.0000       27\n",
      "0.230225 0.187500         4096         4096.0   1.0000   1.0000       27\n",
      "0.215385 0.215385         8192         8192.0  -1.0000  -1.0000       27 h\n",
      "0.198352 0.181319        16384        16384.0   1.0000   1.0000       27 h\n",
      "0.192308 0.186264        32768        32768.0   1.0000   1.0000       27 h\n",
      "0.187200 0.182093        65536        65536.0   1.0000   1.0000       27 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 7200\n",
      "passes used = 11\n",
      "weighted example sum = 79200.000000\n",
      "weighted label sum = 12870.000000\n",
      "average loss = 0.180000 h\n",
      "best constant = 0.162500\n",
      "best constant's loss = 0.973594\n",
      "total feature number = 2138400\n"
     ]
    }
   ],
   "source": [
    "!vw --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only testing\r\n",
      "predictions = data/sentiment.te.pred\r\n",
      "Num weight bits = 18\r\n",
      "learning rate = 0.5\r\n",
      "initial_t = 0\r\n",
      "power_t = 0.5\r\n",
      "using no cache\r\n",
      "Reading datafile = data/sentiment.te\r\n",
      "num sources = 1\r\n",
      "average  since         example        example  current  current  current\r\n",
      "loss     last          counter         weight    label  predict features\r\n",
      "0.000000 0.000000            1            1.0   1.0000   1.0000       27\r\n",
      "0.000000 0.000000            2            2.0   1.0000   1.0000       27\r\n",
      "0.250000 0.500000            4            4.0   1.0000   1.0000       27\r\n",
      "0.250000 0.250000            8            8.0   1.0000   1.0000       27\r\n",
      "0.250000 0.250000           16           16.0  -1.0000  -1.0000       27\r\n",
      "0.281250 0.312500           32           32.0  -1.0000  -1.0000       27\r\n",
      "0.218750 0.156250           64           64.0  -1.0000  -1.0000       27\r\n",
      "0.171875 0.125000          128          128.0   1.0000  -1.0000       27\r\n",
      "0.183594 0.195312          256          256.0  -1.0000  -1.0000       27\r\n",
      "0.185547 0.187500          512          512.0   1.0000   1.0000       27\r\n",
      "0.184570 0.183594         1024         1024.0  -1.0000  -1.0000       27\r\n",
      "\r\n",
      "finished run\r\n",
      "number of examples per pass = 2016\r\n",
      "passes used = 1\r\n",
      "weighted example sum = 2016.000000\r\n",
      "weighted label sum = 336.000000\r\n",
      "average loss = 0.180060\r\n",
      "best constant = 0.166667\r\n",
      "best constant's loss = 0.972222\r\n",
      "total feature number = 54432\r\n"
     ]
    }
   ],
   "source": [
    "!vw --binary -t -i data/sentiment.model -p data/sentiment.te.pred data/sentiment.te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, -1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, 1, 1, -1, 1, -1, -1, 1, -1, -1, 1, 1, -1, 1, -1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, -1, 1, -1, 1, -1, -1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, -1, -1, 1, 1, -1, -1, -1, -1, 1, 1, 1, -1, 1, 1, -1, 1, 1, -1, -1, -1, 1, -1, 1, 1, 1, -1, 1, 1, 1, -1, 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, -1, 1, -1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1, -1, 1, 1, -1, -1, 1, -1, 1, 1, 1, -1, 1, -1, -1, -1, 1, -1, 1, 1, -1, 1, -1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, 1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, -1, -1, -1, -1, -1, 1, -1, 1, 1, -1, 1, 1, -1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, 1, -1, 1, 1, 1, -1, -1, 1, -1, 1, 1, -1, -1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, -1, 1, 1, -1, -1, 1, -1, 1, 1, 1, -1, -1, -1, 1, 1, -1, -1, 1, -1, 1, -1, 1, -1, -1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, -1, 1, -1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, -1, 1, 1, -1, 1, -1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, -1, 1, -1, -1, 1, 1, -1, 1, 1, -1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, -1, 1, -1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, 1, -1, -1, 1, -1, -1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, -1, -1, 1, -1, 1, -1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, -1, -1, 1, 1, 1, 1, -1, 1, -1, -1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, 1, -1, -1, 1, -1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1, -1, 1, 1, -1, -1, 1, -1, -1, -1, 1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1, -1, 1, -1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 1, 1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, 1, -1, -1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, -1, -1, -1, -1, 1, -1, 1, -1, 1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, -1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, -1, -1, 1, 1, 1, -1, -1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, -1, -1, -1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, 1, -1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, -1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, -1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, 1, 1, 1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, 1, -1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, 1, -1, 1, 1, 1, -1, -1, -1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 1, -1, -1, -1, -1, -1, 1, 1, 1, -1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, -1, 1, -1, 1, 1, -1, 1, -1, 1, -1, 1, 1, -1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, -1, 1, -1, 1, 1, 1, 1, -1, 1, -1, 1, -1, -1, 1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, -1, -1, 1, -1, 1, 1, -1, 1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, -1, -1, -1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, -1, 1, -1, 1, -1, -1, 1, -1, 1, -1, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, -1, 1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, 1, 1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, -1, -1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, 1, -1, -1, 1, -1, -1, 1, 1, 1, 1, -1, -1, 1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -1, 1, 1, 1, -1, -1, 1, -1, -1, 1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, -1, 1, -1, 1, 1, -1, 1, 1, -1, 1, -1, -1, 1, 1, -1, -1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, 1, -1, 1, -1, 1, -1, -1, 1, -1, 1, -1, 1, -1, -1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, -1, 1, -1, -1, -1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, -1, -1, 1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, 1, -1, 1, -1, 1, -1, -1, -1, 1, -1]\n"
     ]
    }
   ],
   "source": [
    "# !head data/sentiment.te.pred\n",
    "file = open(\"data/sentiment.te.pred\",\"r\") \n",
    "data = file.read()\n",
    "ugly_arr = data.split('\\n')\n",
    "# print cc\n",
    "predict_value = []\n",
    "for i in ugly_arr:\n",
    "    if i in ['1','-1']:\n",
    "        predict_value.append(int(i))\n",
    "        \n",
    "print predict_value\n",
    "# for i in cc:\n",
    "#     predict_value.append(int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, -1, -1, -1, 1, -1, 1]\n",
      "2016 2016\n",
      "1000 1016 1363 1363\n",
      "in 1-gram precision is 0.496031746032 and recall is 0.423190859077\n"
     ]
    }
   ],
   "source": [
    "# 1 is reformist so p for reformist\n",
    "#-1 is priciplist so n for principlist\n",
    "print predict_value[:10]\n",
    "tp = 0\n",
    "tn = 0\n",
    "print len(predict_value),len(valid_labels)\n",
    "for i in range(len(predict_value)):\n",
    "    if valid_labels[i] == 1:\n",
    "        if predict_value[i] == 1:\n",
    "            tp +=1\n",
    "    else:\n",
    "        if predict_value[i] == -1:\n",
    "            tn +=1\n",
    "fp = len(predict_value) - tp\n",
    "fn = len(predict_value) - tn\n",
    "            \n",
    "print tp,fp,fn,fn\n",
    "\n",
    "precision = float(tp) / (tp + fp)\n",
    "recall = float(tp) / (tp + fn)\n",
    "print \"in 1-gram precision is {0} and recall is {1}\".format(precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, -1, 1, -1, 1, -1, 1, -1, 1]\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "print valid_labels[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = data/sentiment_bigram.model\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = data/sentiment.tr.cache\n",
      "Reading datafile = data/sentiment.tr\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0  -1.0000  -1.0000       52\n",
      "0.500000 1.000000            2            2.0   1.0000  -1.0000       52\n",
      "0.250000 0.000000            4            4.0   1.0000   1.0000       52\n",
      "0.250000 0.250000            8            8.0   1.0000   1.0000       52\n",
      "0.375000 0.500000           16           16.0  -1.0000   1.0000       52\n",
      "0.406250 0.437500           32           32.0   1.0000   1.0000       52\n",
      "0.359375 0.312500           64           64.0  -1.0000   1.0000       52\n",
      "0.375000 0.390625          128          128.0   1.0000   1.0000       52\n",
      "0.335938 0.296875          256          256.0   1.0000   1.0000       52\n",
      "0.359375 0.382812          512          512.0   1.0000   1.0000       52\n",
      "0.308594 0.257812         1024         1024.0  -1.0000  -1.0000       52\n",
      "0.275879 0.243164         2048         2048.0   1.0000   1.0000       52\n",
      "0.232666 0.189453         4096         4096.0   1.0000   1.0000       52\n",
      "0.201099 0.201099         8192         8192.0  -1.0000  -1.0000       52 h\n",
      "0.180769 0.160440        16384        16384.0   1.0000   1.0000       52 h\n",
      "0.172253 0.163736        32768        32768.0   1.0000   1.0000       52 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 7200\n",
      "passes used = 6\n",
      "weighted example sum = 43200.000000\n",
      "weighted label sum = 7020.000000\n",
      "average loss = 0.161250 h\n",
      "best constant = 0.162500\n",
      "best constant's loss = 0.973594\n",
      "total feature number = 2246400\n"
     ]
    }
   ],
   "source": [
    "!vw --binary data/sentiment.tr --ngram 2 --passes 20 -c -k -f data/sentiment_bigram.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for all namespaces.\r\n",
      "Generating 2-grams for all namespaces.\r\n",
      "only testing\r\n",
      "predictions = data/sentiment_bigram.te.pred\r\n",
      "Num weight bits = 18\r\n",
      "learning rate = 0.5\r\n",
      "initial_t = 0\r\n",
      "power_t = 0.5\r\n",
      "using no cache\r\n",
      "Reading datafile = data/sentiment.te\r\n",
      "num sources = 1\r\n",
      "average  since         example        example  current  current  current\r\n",
      "loss     last          counter         weight    label  predict features\r\n",
      "0.000000 0.000000            1            1.0   1.0000   1.0000       52\r\n",
      "0.000000 0.000000            2            2.0   1.0000   1.0000       52\r\n",
      "0.000000 0.000000            4            4.0   1.0000   1.0000       52\r\n",
      "0.000000 0.000000            8            8.0   1.0000   1.0000       52\r\n",
      "0.125000 0.250000           16           16.0  -1.0000  -1.0000       52\r\n",
      "0.218750 0.312500           32           32.0  -1.0000  -1.0000       52\r\n",
      "0.187500 0.156250           64           64.0  -1.0000  -1.0000       52\r\n",
      "0.187500 0.187500          128          128.0   1.0000   1.0000       52\r\n",
      "0.152344 0.117188          256          256.0  -1.0000  -1.0000       52\r\n",
      "0.167969 0.183594          512          512.0   1.0000   1.0000       52\r\n",
      "0.164062 0.160156         1024         1024.0  -1.0000  -1.0000       52\r\n",
      "\r\n",
      "finished run\r\n",
      "number of examples per pass = 2016\r\n",
      "passes used = 1\r\n",
      "weighted example sum = 2016.000000\r\n",
      "weighted label sum = 336.000000\r\n",
      "average loss = 0.164683\r\n",
      "best constant = 0.166667\r\n",
      "best constant's loss = 0.972222\r\n",
      "total feature number = 104832\r\n"
     ]
    }
   ],
   "source": [
    "!vw --binary -t -i data/sentiment_bigram.model --ngram 2 -p data/sentiment_bigram.te.pred data/sentiment.te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !head data/sentiment.te.pred\n",
    "file = open(\"data/sentiment_bigram.te.pred\",\"r\") \n",
    "data = file.read()\n",
    "ugly_arr = data.split('\\n')\n",
    "# print cc\n",
    "predict_bigram_value = []\n",
    "for i in ugly_arr:\n",
    "    if i in ['1','-1']:\n",
    "        predict_bigram_value.append(int(i))\n",
    "        \n",
    "# print predict_value\n",
    "# for i in cc:\n",
    "#     predict_value.append(int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, -1, 1, -1, 1, -1, 1, -1, -1]\n",
      "2016 2016\n",
      "1027 989 1359 1359\n",
      "in 2-gram precision is 0.509424603175 and recall is 0.430427493713\n"
     ]
    }
   ],
   "source": [
    "# 1 is reformist so p for reformist\n",
    "#-1 is priciplist so n for principlist\n",
    "print predict_bigram_value[:10]\n",
    "tp = 0\n",
    "tn = 0\n",
    "print len(predict_bigram_value),len(valid_labels)\n",
    "for i in range(len(predict_bigram_value)):\n",
    "    if valid_labels[i] == 1:\n",
    "        if predict_bigram_value[i] == 1:\n",
    "            tp +=1\n",
    "    else:\n",
    "        if predict_bigram_value[i] == -1:\n",
    "            tn +=1\n",
    "fp = len(predict_bigram_value) - tp\n",
    "fn = len(predict_bigram_value) - tn\n",
    "            \n",
    "print tp,fp,fn,fn\n",
    "\n",
    "precision = float(tp) / (tp + fp)\n",
    "recall = float(tp) / (tp + fn)\n",
    "print \"in 2-gram precision is {0} and recall is {1}\".format(precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 3-grams for all namespaces.\n",
      "final_regressor = data/sentiment_threegram.model\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = data/sentiment.tr.cache\n",
      "Reading datafile = data/sentiment.tr\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0  -1.0000  -1.0000       76\n",
      "0.500000 1.000000            2            2.0   1.0000  -1.0000       76\n",
      "0.250000 0.000000            4            4.0   1.0000   1.0000       76\n",
      "0.250000 0.250000            8            8.0   1.0000   1.0000       76\n",
      "0.375000 0.500000           16           16.0  -1.0000   1.0000       76\n",
      "0.406250 0.437500           32           32.0   1.0000   1.0000       76\n",
      "0.375000 0.343750           64           64.0  -1.0000   1.0000       76\n",
      "0.398438 0.421875          128          128.0   1.0000   1.0000       76\n",
      "0.355469 0.312500          256          256.0   1.0000   1.0000       76\n",
      "0.369141 0.382812          512          512.0   1.0000   1.0000       76\n",
      "0.321289 0.273438         1024         1024.0  -1.0000  -1.0000       76\n",
      "0.277832 0.234375         2048         2048.0   1.0000   1.0000       76\n",
      "0.239746 0.201660         4096         4096.0   1.0000   1.0000       76\n",
      "0.202198 0.202198         8192         8192.0  -1.0000  -1.0000       76 h\n",
      "0.194505 0.186813        16384        16384.0   1.0000   1.0000       76 h\n",
      "0.186264 0.178022        32768        32768.0   1.0000   1.0000       76 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 7200\n",
      "passes used = 6\n",
      "weighted example sum = 43200.000000\n",
      "weighted label sum = 7020.000000\n",
      "average loss = 0.180000 h\n",
      "best constant = 0.162500\n",
      "best constant's loss = 0.973594\n",
      "total feature number = 3283200\n"
     ]
    }
   ],
   "source": [
    "!vw --binary data/sentiment.tr --ngram 3 --passes 20 -c -k -f data/sentiment_threegram.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 3-grams for all namespaces.\r\n",
      "Generating 3-grams for all namespaces.\r\n",
      "only testing\r\n",
      "predictions = data/sentiment_threegram.te.pred\r\n",
      "Num weight bits = 18\r\n",
      "learning rate = 0.5\r\n",
      "initial_t = 0\r\n",
      "power_t = 0.5\r\n",
      "using no cache\r\n",
      "Reading datafile = data/sentiment.te\r\n",
      "num sources = 1\r\n",
      "average  since         example        example  current  current  current\r\n",
      "loss     last          counter         weight    label  predict features\r\n",
      "0.000000 0.000000            1            1.0   1.0000   1.0000       76\r\n",
      "0.000000 0.000000            2            2.0   1.0000   1.0000       76\r\n",
      "0.250000 0.500000            4            4.0   1.0000   1.0000       76\r\n",
      "0.250000 0.250000            8            8.0   1.0000   1.0000       76\r\n",
      "0.250000 0.250000           16           16.0  -1.0000  -1.0000       76\r\n",
      "0.312500 0.375000           32           32.0  -1.0000  -1.0000       76\r\n",
      "0.265625 0.218750           64           64.0  -1.0000  -1.0000       76\r\n",
      "0.210938 0.156250          128          128.0   1.0000   1.0000       76\r\n",
      "0.187500 0.164062          256          256.0  -1.0000  -1.0000       76\r\n",
      "0.207031 0.226562          512          512.0   1.0000   1.0000       76\r\n",
      "0.187500 0.167969         1024         1024.0  -1.0000  -1.0000       76\r\n",
      "\r\n",
      "finished run\r\n",
      "number of examples per pass = 2016\r\n",
      "passes used = 1\r\n",
      "weighted example sum = 2016.000000\r\n",
      "weighted label sum = 336.000000\r\n",
      "average loss = 0.182044\r\n",
      "best constant = 0.166667\r\n",
      "best constant's loss = 0.972222\r\n",
      "total feature number = 153216\r\n"
     ]
    }
   ],
   "source": [
    "!vw --binary -t -i data/sentiment_threegram.model --ngram 3 -p data/sentiment_threegram.te.pred data/sentiment.te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !head data/sentiment.te.pred\n",
    "file = open(\"data/sentiment_threegram.te.pred\",\"r\") \n",
    "data = file.read()\n",
    "ugly_arr = data.split('\\n')\n",
    "# print cc\n",
    "predict_threegram_value = []\n",
    "for i in ugly_arr:\n",
    "    if i in ['1','-1']:\n",
    "        predict_threegram_value.append(int(i))\n",
    "        \n",
    "# print predict_value\n",
    "# for i in cc:\n",
    "#     predict_value.append(int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, -1, -1, -1, 1, -1, -1]\n",
      "2016 2016\n",
      "1027 989 1394 1394\n",
      "in 3-gram precision is 0.509424603175 and recall is 0.424204874019\n"
     ]
    }
   ],
   "source": [
    "# 1 is reformist so p for reformist\n",
    "#-1 is priciplist so n for principlist\n",
    "print predict_threegram_value[:10]\n",
    "tp = 0\n",
    "tn = 0\n",
    "print len(predict_threegram_value),len(valid_labels)\n",
    "for i in range(len(predict_threegram_value)):\n",
    "    if valid_labels[i] == 1:\n",
    "        if predict_threegram_value[i] == 1:\n",
    "            tp +=1\n",
    "    else:\n",
    "        if predict_threegram_value[i] == -1:\n",
    "            tn +=1\n",
    "fp = len(predict_threegram_value) - tp\n",
    "fn = len(predict_threegram_value) - tn\n",
    "            \n",
    "print tp,fp,fn,fn\n",
    "\n",
    "precision = float(tp) / (tp + fp)\n",
    "recall = float(tp) / (tp + fn)\n",
    "print \"in 3-gram precision is {0} and recall is {1}\".format(precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweet",
   "language": "python",
   "name": "tweet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
